{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Shobhit\n",
      "[nltk_data]     Kulshreshtha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Shobhit\n",
      "[nltk_data]     Kulshreshtha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n",
    "\n",
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "# Reading in the dataset\n",
    "data = pd.read_csv('data/hm_train.csv')\n",
    "submission = pd.read_csv('data/hm_test.csv')\n",
    "\n",
    "# Defining a function to lemmatize tokens\n",
    "def lemmatize_sentence(tokens: list) -> str:\n",
    "    \"\"\"\n",
    "    Returns lemmatized sentence in exchange for original sentence tokens\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "def stem_sentence(tokens: list) -> str:\n",
    "    \"\"\"\n",
    "    Returns a stemmed sentence in exchange for original sentence tokens\n",
    "    \"\"\"\n",
    "    # Instantiating a PorterStemmer object\n",
    "    porter = PorterStemmer()\n",
    "    stem_message=[]\n",
    "    for token in tokens:\n",
    "        stem_message.append(porter.stem(token))\n",
    "        stem_message.append(\" \")\n",
    "    return ''.join(stem_message)\n",
    "\n",
    "# Let's define a function to encapsulate all cleaning activities\n",
    "def clean_text(message: str, transform='stem') -> str:\n",
    "    \"\"\"\n",
    "    Performs cleaning & lemmatizationf for a supplied string and returns clean string\n",
    "    transform: 'stem' or 'lemma'\n",
    "    \"\"\"\n",
    "    # Converting to Lowercase\n",
    "    clean_message = message.lower()\n",
    "\n",
    "    STOPWORDS = stopwords.words('english')\n",
    "\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in clean_message if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    clean_message = ''.join(nopunc)\n",
    "\n",
    "    # Now just remove any stopwords\n",
    "    clean_message = ' '.join([word for word in clean_message.split() if word not in STOPWORDS])\n",
    "    \n",
    "    if(transform == 'stem'):\n",
    "        # Stem words using the Stemming function\n",
    "        return ''.join(stem_sentence(word_tokenize(clean_message)))\n",
    "    else:\n",
    "        # Lemmatize words using the Lemmatize function\n",
    "        return ''.join(lemmatize_sentence(word_tokenize(clean_message)))\n",
    "\n",
    "\n",
    "def build_model(clf, data, textcol: str, targetcol: str, clean=False, text_transform='stem'):\n",
    "    \"\"\"\n",
    "    Returns the classification report and built model based on data & inputs supplied\n",
    "    text_transform: 'stem' or 'lemma'\n",
    "    \"\"\"\n",
    "    if(clean):\n",
    "        X = data[textcol].apply(clean_text, transform = text_transform)\n",
    "    else:\n",
    "        X = data[textcol]\n",
    "    y = data[targetcol]\n",
    "\n",
    "    # Split into test and training\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "    # Create and fit pipeline for text data\n",
    "    pipe = Pipeline([('bow', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', clf)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Print the output on test data\n",
    "    print(\"Classification Report Test Data\")\n",
    "    print(metrics.classification_report(y_test, pipe.predict(X_test)))\n",
    "\n",
    "    # Return the built model\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report Test Data\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "     achievement       0.79      0.89      0.83      4028\n",
      "       affection       0.62      0.96      0.75      4227\n",
      "         bonding       0.98      0.30      0.46      1315\n",
      "enjoy_the_moment       0.85      0.24      0.37      1287\n",
      "        exercise       0.00      0.00      0.00       146\n",
      "         leisure       0.95      0.25      0.40       823\n",
      "          nature       0.00      0.00      0.00       239\n",
      "\n",
      "        accuracy                           0.71     12065\n",
      "       macro avg       0.60      0.38      0.40     12065\n",
      "    weighted avg       0.74      0.71      0.66     12065\n",
      "\n",
      "Classification Report Test Data\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "     achievement       0.89      0.96      0.92      4028\n",
      "       affection       0.95      0.97      0.96      4227\n",
      "         bonding       0.97      0.92      0.94      1315\n",
      "enjoy_the_moment       0.86      0.78      0.81      1287\n",
      "        exercise       0.98      0.64      0.78       146\n",
      "         leisure       0.88      0.78      0.83       823\n",
      "          nature       0.92      0.75      0.82       239\n",
      "\n",
      "        accuracy                           0.92     12065\n",
      "       macro avg       0.92      0.83      0.87     12065\n",
      "    weighted avg       0.92      0.92      0.92     12065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initializing models to use\n",
    "mnb = MultinomialNB()\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Training models\n",
    "NB = build_model(mnb, data, 'cleaned_hm', 'predicted_category', clean=False)\n",
    "LR = build_model(logreg, data, 'cleaned_hm', 'predicted_category', clean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating predictions\n",
    "output_nb = NB.predict(submission['cleaned_hm']) #Not cleaning as clean was set to False\n",
    "output_lr = LR.predict(submission['cleaned_hm']) #Not cleaning as clean was set to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating output for submission\n",
    "# Naive Bayes\n",
    "submission = pd.concat([submission['hmid'],pd.Series(output_nb)], axis=1)\n",
    "submission = submission.rename({0:'predicted_category'}, axis=1)\n",
    "submission.to_csv('submission_1.csv',index=False)\n",
    "# Logistic Regression\n",
    "submission = pd.concat([submission['hmid'],pd.Series(output_lr)], axis=1)\n",
    "submission = submission.rename({0:'predicted_category'}, axis=1)\n",
    "submission.to_csv('submission_2.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
